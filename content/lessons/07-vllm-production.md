# Production Inference with vLLM

**âš ï¸ Note:** vLLM requires the HuggingFace model format. If you downloaded the model in Lesson 3 before this update, you may need to re-download to get both Meta and HuggingFace formats. The latest Lesson 3 downloads the complete model with all formats.

Take your AI deployment to the next level with vLLM - a production-grade inference engine that provides OpenAI-compatible APIs, continuous batching, and enterprise features for Tenstorrent hardware.

## What is vLLM?

**vLLM** is an open-source LLM serving library designed for high-throughput, low-latency inference. Tenstorrent maintains a fork that brings vLLM's advanced features to Tenstorrent hardware.

**Why vLLM?**
- ğŸš€ **OpenAI-compatible API** - drop-in replacement for OpenAI's API
- âš¡ **Continuous batching** - efficiently serve multiple users simultaneously
- ğŸ“Š **Production-tested** - used by companies at scale
- ğŸ”§ **Advanced features** - request queuing, priority scheduling, streaming
- ğŸ¯ **Easy deployment** - standardized server interface

## Journey So Far

- **Lesson 3:** One-shot inference demo
- **Lesson 4:** Interactive chat (custom app, model in memory)
- **Lesson 5:** Flask HTTP API (basic server)
- **Lesson 6:** vLLM (production-grade serving) â† **You are here**

## vLLM vs. Your Flask Server

| Feature | Flask (Lesson 5) | vLLM (Lesson 6) |
|---------|------------------|-----------------|
| Model Loading | Manual | Automatic |
| API Compatibility | Custom | OpenAI-compatible |
| Multiple Users | Sequential | Continuous batching |
| Request Queuing | Manual | Built-in |
| Streaming | Manual | Built-in |
| Production-Ready | Basic | Enterprise-grade |
| Learning Curve | Easy | Moderate |

**When to use what:**
- **Flask (Lesson 5):** Learning, prototyping, simple use cases
- **vLLM (Lesson 6):** Production, multiple users, scalability

## Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚       vLLM Server                â”‚
â”‚                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  OpenAI-Compatible API     â”‚  â”‚  â† Drop-in replacement
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  Continuous Batch Engine   â”‚  â”‚  â† Efficient multi-user
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  TT-Metal Backend          â”‚  â”‚  â† Your hardware
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â†•
   OpenAI Python SDK
   curl / HTTP clients
   Your applications
```

## Prerequisites

- tt-metal installed and working (latest main branch - see Step 0 below if you need to update)
- Model downloaded (Llama-3.1-8B-Instruct)
- Python 3.10+ recommended
- ~20GB disk space for vLLM installation

## Starting Fresh?

If you're jumping directly to this lesson, verify your setup first:

**Quick prerequisite checks:**
```bash
# Hardware detected?
tt-smi

# tt-metal working?
python3 -c "import ttnn; print('âœ“ tt-metal ready')"

# Model downloaded?
ls ~/models/Llama-3.1-8B-Instruct/config.json

# Python version?
python3 --version  # Need 3.10+
```

**If any checks fail:**
- **No hardware?** â†’ See [Lesson 1: Hardware Detection](#)
- **No tt-metal?** â†’ See [Lesson 2: Verify Installation](#)
- **No model?** â†’ See [Lesson 3: Download Model](#) or download now:
  ```bash
  huggingface-cli download meta-llama/Llama-3.1-8B-Instruct \
    --local-dir ~/models/Llama-3.1-8B-Instruct
  ```

---

## Hardware Configuration

**Quick Check:** Not sure which hardware you have? Run this command to detect your device:

[ğŸ” Detect Hardware](command:tenstorrent.runHardwareDetection)

Look for the "Board Type" field in the output (e.g., n150, n300, t3k, p100).

---

**Choose your hardware configuration below:**

<details open style="border: 1px solid var(--vscode-panel-border); border-radius: 6px; padding: 12px; margin: 8px 0; background: var(--vscode-editor-background);">
<summary style="cursor: pointer; font-weight: bold; padding: 4px; margin: -12px -12px 12px -12px; background: var(--vscode-sideBar-background); border-radius: 4px 4px 0 0; border-bottom: 1px solid var(--vscode-panel-border);"><b>ğŸ”§ N150 (Wormhole - Single Chip)</b> - Most common for development</summary>

**Specifications:**
- Chips: 1
- Model: Llama-3.1-8B-Instruct
- Context Length: 64K tokens
- Best for: Development, single-user deployments, learning

**Environment Variables:**
```bash
export MESH_DEVICE=N150
export TT_METAL_HOME=~/tt-metal
export PYTHONPATH=$TT_METAL_HOME:$PYTHONPATH
```

**vLLM Flags (use in Step 4):**
```bash
--max-model-len 65536    # 64K context
--max-num-seqs 16        # Concurrent sequences
--block-size 64          # KV cache block size
```

</details>

<details style="border: 1px solid var(--vscode-panel-border); border-radius: 6px; padding: 12px; margin: 8px 0; background: var(--vscode-editor-background);">
<summary style="cursor: pointer; font-weight: bold; padding: 4px; margin: -12px -12px 12px -12px; background: var(--vscode-sideBar-background); border-radius: 4px 4px 0 0; border-bottom: 1px solid var(--vscode-panel-border);"><b>ğŸ”§ N300 (Wormhole - Dual Chip)</b></summary>

**Specifications:**
- Chips: 2
- Models: Llama-3.1-8B-Instruct OR Qwen-2.5-7B-Coder
- Context Length: 128K tokens
- Tensor Parallelism: TP=2 (uses both chips)
- Best for: Higher throughput, production deployments

**Environment Variables:**
```bash
export MESH_DEVICE=N300
export TT_METAL_HOME=~/tt-metal
export PYTHONPATH=$TT_METAL_HOME:$PYTHONPATH
```

**vLLM Flags (use in Step 4):**
```bash
--max-model-len 131072   # 128K context
--max-num-seqs 32        # More concurrent sequences
--block-size 64
--tensor-parallel-size 2 # Use both chips
```

</details>

<details style="border: 1px solid var(--vscode-panel-border); border-radius: 6px; padding: 12px; margin: 8px 0; background: var(--vscode-editor-background);">
<summary style="cursor: pointer; font-weight: bold; padding: 4px; margin: -12px -12px 12px -12px; background: var(--vscode-sideBar-background); border-radius: 4px 4px 0 0; border-bottom: 1px solid var(--vscode-panel-border);"><b>ğŸ”§ T3K (Wormhole - 8 Chips)</b></summary>

**Specifications:**
- Chips: 8
- Model: Llama-3.1-70B-Instruct (requires 70B model download)
- Context Length: 128K+ tokens
- Tensor Parallelism: TP=8 (uses all chips)
- Best for: Large models (70B+), multi-user production

**Environment Variables:**
```bash
export MESH_DEVICE=T3K
export TT_METAL_HOME=~/tt-metal
export PYTHONPATH=$TT_METAL_HOME:$PYTHONPATH
```

**vLLM Flags (use in Step 4):**
```bash
--model ~/models/Llama-3.1-70B-Instruct  # Note: 70B model!
--max-model-len 131072
--max-num-seqs 64
--block-size 64
--tensor-parallel-size 8
```

**Note:** T3K supports larger models like Llama-3.1-70B. Download the 70B model separately if you want to use it.

</details>

<details style="border: 1px solid var(--vscode-panel-border); border-radius: 6px; padding: 12px; margin: 8px 0; background: var(--vscode-editor-background);">
<summary style="cursor: pointer; font-weight: bold; padding: 4px; margin: -12px -12px 12px -12px; background: var(--vscode-sideBar-background); border-radius: 4px 4px 0 0; border-bottom: 1px solid var(--vscode-panel-border);"><b>ğŸ”§ P100 (Blackhole - Single Chip)</b></summary>

**Specifications:**
- Chips: 1 (newer Blackhole architecture)
- Model: Llama-3.1-8B-Instruct
- Context Length: 64K tokens
- Best for: Development with latest hardware

**Environment Variables:**
```bash
export MESH_DEVICE=P100
export TT_METAL_ARCH_NAME=blackhole  # âš ï¸ Required for Blackhole
export TT_METAL_HOME=~/tt-metal
export PYTHONPATH=$TT_METAL_HOME:$PYTHONPATH
```

**vLLM Flags (use in Step 4):**
```bash
--max-model-len 65536
--max-num-seqs 16
--block-size 64
```

**âš ï¸ Critical:** Blackhole hardware (P100) requires `TT_METAL_ARCH_NAME=blackhole` environment variable. Without this, device detection will fail.

**Note:** P100 support may be experimental. See "Alternative: Use TT-Jukebox" below for validated configurations.

</details>

---

**ğŸ’¡ Tip:** If you're unsure, start with N150 configuration - it works on most hardware, just with potentially different performance characteristics.

## Step 0: Update and Build TT-Metal (If Needed)

**âš ï¸ Important:** vLLM dev branch requires the latest tt-metal. If you get an `InputRegistry` error or "sfpi not found" error, update and rebuild tt-metal:

```bash
cd ~/tt-metal && \
  git checkout main && \
  git pull origin main && \
  git submodule update --init --recursive && \
  ./install_dependencies.sh && \
  ./build_metal.sh
```

[ğŸ”§ Update and Build TT-Metal](command:tenstorrent.updateTTMetal)

**What this does:**
- Updates tt-metal to latest main branch
- Updates all submodules (including SFPI libraries)
- **Installs/updates system dependencies** (libraries, drivers, build tools)
- Rebuilds tt-metal with latest changes
- Takes ~5-15 minutes depending on hardware and system state

**When to do this:**
- First time setting up vLLM
- After updating tt-metal with `git pull`
- If you see "sfpi not found" errors
- If you see "InputRegistry" or other API compatibility errors
- After system updates or fresh installations

**Why install_dependencies.sh?** tt-metal requires specific system libraries, kernel modules, and build tools. This script ensures all dependencies are installed before building. Skipping this step can cause build failures or runtime errors.

**Why rebuild?** tt-metal includes compiled components (like SFPI) that must be built after code updates. The `build_metal.sh` script handles all necessary compilation steps.

---

## Verify vLLM Components

Before proceeding, let's check what you already have installed:

```bash
# Check if vLLM is cloned
[ -d ~/tt-vllm ] && echo "âœ“ vLLM repo found" || echo "âœ— vLLM repo missing"

# Check if venv exists
[ -d ~/tt-vllm-venv ] && echo "âœ“ vLLM venv found" || echo "âœ— vLLM venv missing"

# Check if server script exists
[ -f ~/tt-scratchpad/start-vllm-server.py ] && echo "âœ“ Server script found" || echo "âœ— Server script missing"
```

**All checks passed?** You can skip to [Step 4: Start the Server](#step-4-start-the-openai-compatible-server).

**Some checks failed?** Continue with the steps below - they'll create missing components.

---

## Step 1: Clone TT vLLM Fork

First, get Tenstorrent's vLLM fork:

```bash
cd ~ && \
  git clone --branch dev https://github.com/tenstorrent/vllm.git tt-vllm && \
  cd tt-vllm
```

[ğŸ“¦ Clone TT vLLM Repository](command:tenstorrent.cloneVllm)

**What this does:**
- Clones the `dev` branch (Tenstorrent's main branch)
- Creates `~/tt-vllm` directory
- Takes ~1-2 minutes depending on connection

## Step 2: Set Up vLLM Environment

Create a dedicated virtual environment and install vLLM with all required dependencies.

**This command will:**
- Create a Python virtual environment (~30 seconds)
- Install vLLM and dependencies (~5-10 minutes)
- Configure Tenstorrent hardware support

```bash
cd ~/tt-vllm && \
  python3 -m venv ~/tt-vllm-venv && \
  source ~/tt-vllm-venv/bin/activate && \
  pip install --upgrade pip && \
  export vllm_dir=$(pwd) && \
  source $vllm_dir/tt_metal/setup-metal.sh && \
  pip install --upgrade ttnn pytest && \
  pip install fairscale termcolor loguru blobfile fire pytz llama-models==0.0.48 && \
  pip install -e . --extra-index-url https://download.pytorch.org/whl/cpu
```

[âš™ï¸ Install vLLM](command:tenstorrent.installVllm)

**What happens:**
- Creates `~/tt-vllm-venv` (isolated from your other Python packages)
- Upgrades pip
- Sources tt-metal setup for Tenstorrent support
- Installs ttnn and pytest (required for Tenstorrent hardware)
- Installs dependencies: fairscale, termcolor, loguru, blobfile, fire, pytz
- Installs llama-models==0.0.48 (Tenstorrent's Llama implementation)
- Installs vLLM in editable mode (you can modify it if needed)

**Why a separate venv?** Prevents dependency conflicts with other Python environments. Each stays clean and isolated.

**Time estimate:** ~5-10 minutes total

## Step 3: Run Offline Inference (Optional - Skip for N150)

Test vLLM with a simple offline inference example:

```bash
cd ~/tt-vllm && \
  source ~/tt-vllm-venv/bin/activate && \
  export TT_METAL_HOME=~/tt-metal && \
  export HF_MODEL=~/models/Llama-3.1-8B-Instruct && \
  export MESH_DEVICE=N150 && \
  export PYTHONPATH=$TT_METAL_HOME:$PYTHONPATH && \
  source ~/tt-vllm/tt_metal/setup-metal.sh && \
  python examples/offline_inference_tt.py
```

[ğŸ§ª Run Offline Inference](command:tenstorrent.runVllmOffline)

**âš ï¸ N150 Note:** The offline inference script may show warnings about context length (128K default). This is expected - the script is primarily for testing larger hardware. **You can skip this step and go straight to Step 4 (API server)** where you can properly configure the 64K limit.

**Important:** `TT_METAL_HOME` and `PYTHONPATH` are required so vLLM can find the TTLlamaForCausalLM model implementation.

**What you might see:**
```
WARNING: The model has a long context length (131072). This may cause OOM...
```

This warning is safe to ignore - it's just telling you the model *supports* 128K, but we'll configure 64K in the API server.

**What you'll see:**

```
Loading model...
Model loaded successfully!

Prompt: 'Hello, my name is'
Generated text: ' John. I am a software engineer...'

Prompt: 'The capital of France is'
Generated text: ' Paris. It is known for the Eiffel Tower...'

...
```

**This demonstrates:**
- vLLM can load your model
- Basic inference works
- ~20-40 tokens/second generation speed

## Step 4: Start the OpenAI-Compatible Server

Now start vLLM as an HTTP server with OpenAI-compatible endpoints.

**âš ï¸ Important:** Use the configuration for your hardware from the [Hardware Configuration](#hardware-configuration) section above.

**Choose your hardware below to see the exact command:**

<details open style="border: 1px solid var(--vscode-panel-border); border-radius: 6px; padding: 12px; margin: 8px 0; background: var(--vscode-editor-background);">
<summary style="cursor: pointer; font-weight: bold; padding: 4px; margin: -12px -12px 12px -12px; background: var(--vscode-sideBar-background); border-radius: 4px 4px 0 0; border-bottom: 1px solid var(--vscode-panel-border);"><b>ğŸ”§ N150 (Wormhole - Single Chip)</b></summary>

```bash
cd ~/tt-vllm && \
  source ~/tt-vllm-venv/bin/activate && \
  export TT_METAL_HOME=~/tt-metal && \
  export MESH_DEVICE=N150 && \
  export PYTHONPATH=$TT_METAL_HOME:$PYTHONPATH && \
  source ~/tt-vllm/tt_metal/setup-metal.sh && \
  python ~/tt-scratchpad/start-vllm-server.py \
    --model ~/models/Llama-3.1-8B-Instruct \
    --host 0.0.0.0 \
    --port 8000 \
    --max-model-len 65536 \
    --max-num-seqs 16 \
    --block-size 64
```

[ğŸš€ Start vLLM Server (N150)](command:tenstorrent.startVllmServerN150)

</details>

<details style="border: 1px solid var(--vscode-panel-border); border-radius: 6px; padding: 12px; margin: 8px 0; background: var(--vscode-editor-background);">
<summary style="cursor: pointer; font-weight: bold; padding: 4px; margin: -12px -12px 12px -12px; background: var(--vscode-sideBar-background); border-radius: 4px 4px 0 0; border-bottom: 1px solid var(--vscode-panel-border);"><b>ğŸ”§ N300 (Wormhole - Dual Chip)</b></summary>

```bash
cd ~/tt-vllm && \
  source ~/tt-vllm-venv/bin/activate && \
  export TT_METAL_HOME=~/tt-metal && \
  export MESH_DEVICE=N300 && \
  export PYTHONPATH=$TT_METAL_HOME:$PYTHONPATH && \
  source ~/tt-vllm/tt_metal/setup-metal.sh && \
  python ~/tt-scratchpad/start-vllm-server.py \
    --model ~/models/Llama-3.1-8B-Instruct \
    --host 0.0.0.0 \
    --port 8000 \
    --max-model-len 131072 \
    --max-num-seqs 32 \
    --block-size 64 \
    --tensor-parallel-size 2
```

[ğŸš€ Start vLLM Server (N300)](command:tenstorrent.startVllmServerN300)

</details>

<details style="border: 1px solid var(--vscode-panel-border); border-radius: 6px; padding: 12px; margin: 8px 0; background: var(--vscode-editor-background);">
<summary style="cursor: pointer; font-weight: bold; padding: 4px; margin: -12px -12px 12px -12px; background: var(--vscode-sideBar-background); border-radius: 4px 4px 0 0; border-bottom: 1px solid var(--vscode-panel-border);"><b>ğŸ”§ T3K (Wormhole - 8 Chips)</b></summary>

```bash
cd ~/tt-vllm && \
  source ~/tt-vllm-venv/bin/activate && \
  export TT_METAL_HOME=~/tt-metal && \
  export MESH_DEVICE=T3K && \
  export PYTHONPATH=$TT_METAL_HOME:$PYTHONPATH && \
  source ~/tt-vllm/tt_metal/setup-metal.sh && \
  python ~/tt-scratchpad/start-vllm-server.py \
    --model ~/models/Llama-3.1-70B-Instruct \
    --host 0.0.0.0 \
    --port 8000 \
    --max-model-len 131072 \
    --max-num-seqs 64 \
    --block-size 64 \
    --tensor-parallel-size 8
```

[ğŸš€ Start vLLM Server (T3K)](command:tenstorrent.startVllmServerT3K)

**Note:** This uses the 70B model. Make sure you've downloaded it first.

</details>

<details style="border: 1px solid var(--vscode-panel-border); border-radius: 6px; padding: 12px; margin: 8px 0; background: var(--vscode-editor-background);">
<summary style="cursor: pointer; font-weight: bold; padding: 4px; margin: -12px -12px 12px -12px; background: var(--vscode-sideBar-background); border-radius: 4px 4px 0 0; border-bottom: 1px solid var(--vscode-panel-border);"><b>ğŸ”§ P100 (Blackhole - Single Chip)</b></summary>

```bash
cd ~/tt-vllm && \
  source ~/tt-vllm-venv/bin/activate && \
  export TT_METAL_HOME=~/tt-metal && \
  export MESH_DEVICE=P100 && \
  export TT_METAL_ARCH_NAME=blackhole && \
  export PYTHONPATH=$TT_METAL_HOME:$PYTHONPATH && \
  source ~/tt-vllm/tt_metal/setup-metal.sh && \
  python ~/tt-scratchpad/start-vllm-server.py \
    --model ~/models/Llama-3.1-8B-Instruct \
    --host 0.0.0.0 \
    --port 8000 \
    --max-model-len 65536 \
    --max-num-seqs 16 \
    --block-size 64
```

[ğŸš€ Start vLLM Server (P100)](command:tenstorrent.startVllmServerP100)

**âš ï¸ Remember:** P100 requires `TT_METAL_ARCH_NAME=blackhole` environment variable.

</details>

---

### Why a Custom Starter Script?

**The Problem:** vLLM doesn't automatically know about Tenstorrent's custom model implementations (like `TTLlamaForCausalLM`). Without registration, vLLM will fail with:
```
ValidationError: Cannot find model module 'TTLlamaForCausalLM'
```

**The Solution:** A production-ready starter script that:
1. **Registers TT models** with vLLM's `ModelRegistry` API before the server starts
2. **Self-contained** - No dependency on fragile `examples/` directory
3. **Production-ready** - Can be version controlled, deployed, and maintained

**What the script does:**
```python
from vllm import ModelRegistry

# Register TT Llama implementation
ModelRegistry.register_model(
    "TTLlamaForCausalLM",
    "models.tt_transformers.tt.generator_vllm:LlamaForCausalLM"
)

# Then start vLLM server with all your flags
```

**Why not use `python -m vllm.entrypoints.openai.api_server` directly?**
- âŒ TT models not registered â†’ ValidationError
- âŒ Falls back to slow HuggingFace Transformers (CPU)
- âŒ No way to register via CLI flags or environment variables

**Why not import from examples/?**
- âŒ `examples/` is not production code (may change/move/break)
- âŒ Creates fragile dependency on repository structure
- âŒ Not suitable for deployment or version control

**âœ… Our approach:** Self-contained, production-ready script with inline registration

**The extension creates this script automatically** when you click the button above. You can also view/customize it at `~/tt-scratchpad/start-vllm-server.py`.

---

### Understanding the Configuration

**Environment variables (all hardware types need these):**
- `TT_METAL_HOME=~/tt-metal` - Points to tt-metal installation (required by setup-metal.sh)
- `MESH_DEVICE=<your-hardware>` - Targets your specific hardware (N150, N300, T3K, P100)
- `TT_METAL_ARCH_NAME=<architecture>` - **Required for Blackhole (P100)**: Set to `blackhole`. Wormhole chips (N150/N300/T3K) auto-detect but P100 needs explicit specification.
- `PYTHONPATH=$TT_METAL_HOME` - Required so Python can import TT model classes from tt-metal

**vLLM flags (vary by hardware):**
- `--model` - Local model path (downloaded in Lesson 3)
- `--max-model-len` - Context limit (64K for single-chip, 128K for multi-chip)
- `--max-num-seqs` - Maximum concurrent sequences (higher on multi-chip)
- `--block-size` - KV cache block size (typically 64)
- `--tensor-parallel-size` - Number of chips to use (only for multi-chip)

**What you'll see:**

```
INFO: Loading model meta-llama/Llama-3.1-8B-Instruct
INFO: Initializing TT-Metal backend...
INFO: Model loaded successfully
INFO: Started server process
INFO: Waiting for application startup.
INFO: Application startup complete.
INFO: Uvicorn running on http://0.0.0.0:8000 (Press CTRL+C to quit)
```

**Server is ready!** Leave this terminal open.

## Step 5: Test with OpenAI SDK

Open a **second terminal** and test with the OpenAI Python SDK:

```python
# Install OpenAI SDK if needed
# pip install openai

from openai import OpenAI

# Point to your vLLM server
client = OpenAI(
    base_url="http://localhost:8000/v1",
    api_key="dummy-key"  # vLLM doesn't require auth by default
)

# Chat completion
response = client.chat.completions.create(
    model="meta-llama/Llama-3.1-8B-Instruct",
    messages=[
        {"role": "user", "content": "What is machine learning?"}
    ],
    max_tokens=128
)

print(response.choices[0].message.content)
```

[ğŸ’¬ Test with OpenAI SDK](command:tenstorrent.testVllmOpenai)

**Response:**
```
Machine learning is a subset of artificial intelligence that involves
training algorithms to learn from data and make predictions...
```

**Why this is powerful:** Your code is **identical** to code that calls OpenAI's API. Just change the `base_url`!

## Step 6: Test with curl

You can also use curl (same API as OpenAI):

```bash
curl http://localhost:8000/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "meta-llama/Llama-3.1-8B-Instruct",
    "messages": [
      {"role": "user", "content": "Explain neural networks"}
    ],
    "max_tokens": 128
  }'
```

[ğŸ”§ Test with curl](command:tenstorrent.testVllmCurl)

**Response:**
```json
{
  "id": "cmpl-xxx",
  "object": "chat.completion",
  "created": 1234567890,
  "model": "meta-llama/Llama-3.1-8B-Instruct",
  "choices": [
    {
      "index": 0,
      "message": {
        "role": "assistant",
        "content": "Neural networks are computing systems inspired by..."
      },
      "finish_reason": "stop"
    }
  ],
  "usage": {
    "prompt_tokens": 5,
    "completion_tokens": 45,
    "total_tokens": 50
  }
}
```

## OpenAI-Compatible Endpoints

vLLM implements the OpenAI API specification:

### POST /v1/chat/completions

Chat-style completions (like ChatGPT):

```bash
curl http://localhost:8000/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "meta-llama/Llama-3.1-8B-Instruct",
    "messages": [
      {"role": "system", "content": "You are a helpful assistant."},
      {"role": "user", "content": "What is AI?"}
    ],
    "temperature": 0.7,
    "max_tokens": 256
  }'
```

### POST /v1/completions

Text completions (continue a prompt):

```bash
curl http://localhost:8000/v1/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "meta-llama/Llama-3.1-8B-Instruct",
    "prompt": "Once upon a time",
    "max_tokens": 100
  }'
```

### GET /v1/models

List available models:

```bash
curl http://localhost:8000/v1/models
```

Response:
```json
{
  "object": "list",
  "data": [
    {
      "id": "meta-llama/Llama-3.1-8B-Instruct",
      "object": "model",
      "owned_by": "tenstorrent"
    }
  ]
}
```

## Streaming Responses

vLLM supports streaming (tokens arrive as they're generated):

```python
from openai import OpenAI

client = OpenAI(base_url="http://localhost:8000/v1", api_key="dummy")

stream = client.chat.completions.create(
    model="meta-llama/Llama-3.1-8B-Instruct",
    messages=[{"role": "user", "content": "Write a story"}],
    stream=True,  # Enable streaming
    max_tokens=200
)

for chunk in stream:
    if chunk.choices[0].delta.content is not None:
        print(chunk.choices[0].delta.content, end='', flush=True)
```

Output appears word-by-word as it's generated!

## Continuous Batching Demo

vLLM's killer feature: serve multiple users efficiently:

```python
import asyncio
from openai import OpenAI

client = OpenAI(base_url="http://localhost:8000/v1", api_key="dummy")

async def query(prompt_id, prompt):
    """Send a query"""
    print(f"[{prompt_id}] Sending request...")
    response = client.chat.completions.create(
        model="meta-llama/Llama-3.1-8B-Instruct",
        messages=[{"role": "user", "content": prompt}],
        max_tokens=50
    )
    print(f"[{prompt_id}] Got response: {response.choices[0].message.content[:50]}...")

async def main():
    """Send 5 requests simultaneously"""
    tasks = [
        query(1, "What is AI?"),
        query(2, "Explain Python"),
        query(3, "What is quantum computing?"),
        query(4, "Tell me about space"),
        query(5, "How do computers work?")
    ]
    await asyncio.gather(*tasks)

asyncio.run(main())
```

**vLLM handles all 5 requests efficiently** using continuous batching - much better than sequential processing!

## Advanced Configuration

### Custom Parameters

```bash
python -m vllm.entrypoints.openai.api_server \
  --model meta-llama/Llama-3.1-8B-Instruct \
  --host 0.0.0.0 \
  --port 8000 \
  --max-model-len 2048 \           # Max sequence length
  --max-num-seqs 16 \              # Max concurrent sequences
  --disable-log-requests \          # Reduce logging
  --trust-remote-code              # Allow custom models
```

### Environment Variables

```bash
# Control tensor parallelism
export MESH_DEVICE=T3K  # or N150, N300, etc.

# Set cache directory
export HF_HOME=~/hf_cache

# Enable debug logging
export VLLM_LOGGING_LEVEL=DEBUG
```

## Deployment Patterns

### Pattern 1: Single Server

Simple deployment for moderate load:

```bash
python -m vllm.entrypoints.openai.api_server \
  --model $HF_MODEL \
  --host 0.0.0.0 \
  --port 8000
```

**Good for:** Dev/test, small teams, moderate QPS

### Pattern 2: Docker Container

Containerized deployment:

```dockerfile
FROM tenstorrent/tt-metal:latest

RUN pip install vllm

CMD python -m vllm.entrypoints.openai.api_server \
    --model meta-llama/Llama-3.1-8B-Instruct \
    --host 0.0.0.0 \
    --port 8000
```

**Good for:** Consistent environments, easier scaling

### Pattern 3: Load Balanced

Multiple vLLM servers behind nginx:

```
nginx (load balancer)
  â”œâ”€â”€ vLLM server 1 (port 8001)
  â”œâ”€â”€ vLLM server 2 (port 8002)
  â””â”€â”€ vLLM server 3 (port 8003)
```

**Good for:** High availability, horizontal scaling

## Performance Tuning

**Tips for best performance:**

1. **Set appropriate batch size:**
```bash
--max-num-seqs 32  # Higher = more throughput, more memory
```

2. **Optimize sequence length:**
```bash
--max-model-len 2048  # Match your use case
```

3. **Enable GPU memory optimization:**
```bash
--gpu-memory-utilization 0.9  # Use 90% of GPU memory
```

4. **Monitor metrics:**
- Watch request latency
- Track throughput (requests/sec)
- Monitor GPU/NPU utilization

## Monitoring and Observability

vLLM provides metrics endpoints:

```bash
# Prometheus metrics
curl http://localhost:8000/metrics

# Health check
curl http://localhost:8000/health

# Server stats
curl http://localhost:8000/v1/models
```

**Integration with monitoring tools:**
- Prometheus for metrics collection
- Grafana for visualization
- Custom alerting on latency/throughput

## Comparison: Your Journey

| Approach | Speed | Control | Prod-Ready | Use Case |
|----------|-------|---------|------------|----------|
| **Lesson 3: One-shot** | Slow | Low | âŒ | Testing |
| **Lesson 4: Direct API** | Fast | High | âš ï¸ | Learning |
| **Lesson 5: Flask** | Fast | High | âš ï¸ | Prototyping |
| **Lesson 6: vLLM** | Fast | Medium | âœ… | Production |

**Summary:**
- **Lessons 3-4:** Learn how inference works
- **Lesson 5:** Build custom APIs
- **Lesson 6:** Deploy at scale

Each approach serves a purpose - choose based on your needs.

## Troubleshooting

Don't worry if you hit issues - they're usually straightforward to fix. Here are common solutions:

### Server Won't Start

**Check your environment:**
```bash
# Activate venv
source ~/tt-vllm-venv/bin/activate

# Source tt-metal setup
source ~/tt-vllm/tt_metal/setup-metal.sh

# Verify model path
ls ~/models/Llama-3.1-8B-Instruct/config.json
```

**Import errors (e.g., "No module named 'llama_models'", "No module named 'fairscale'", "No module named 'pytz'", etc.):**
```bash
# Install all required dependencies in the venv
source ~/tt-vllm-venv/bin/activate
pip install --upgrade ttnn pytest
pip install fairscale termcolor loguru blobfile fire pytz llama-models==0.0.48
```

**AttributeError: 'InputRegistry' object has no attribute 'register_input_processor':**
**Error: sfpi not found at /home/user/tt-metal/runtime/sfpi:**
These errors indicate tt-metal needs to be updated and rebuilt. Solution:
```bash
# Update and rebuild tt-metal (Step 0)
cd ~/tt-metal
./build_metal.sh --clean       # Clean old build artifacts first
git checkout main
git pull origin main
git submodule update --init --recursive
./install_dependencies.sh      # Install/update system dependencies
./build_metal.sh               # Build tt-metal

# Then upgrade ttnn in vLLM venv
source ~/tt-vllm-venv/bin/activate
pip install --upgrade ttnn
```

**Why `--clean`?** Removes all cached build artifacts to prevent conflicts between old and new versions. This forces a complete rebuild from scratch.

**Why install_dependencies.sh?** Ensures all system libraries, kernel modules, and build tools are installed before building. Prevents build failures and runtime errors.

**Why rebuild?** tt-metal includes compiled components (SFPI libraries, kernels) that must be built after code updates. The vLLM dev branch expects the latest tt-metal APIs.

**RuntimeError: Failed to infer device type (Blackhole P100):**
If you see `RuntimeError: Failed to infer device type`, you need to explicitly set the architecture:
```bash
export TT_METAL_ARCH_NAME=blackhole
export MESH_DEVICE=P100
```

**Why this happens:** Blackhole hardware (P100) requires explicit architecture specification. Wormhole chips (N150/N300/T3K) auto-detect, but Blackhole needs `TT_METAL_ARCH_NAME=blackhole`.

**Full Blackhole startup command:**
```bash
cd ~/tt-vllm && \
  source ~/tt-vllm-venv/bin/activate && \
  export TT_METAL_HOME=~/tt-metal && \
  export MESH_DEVICE=P100 && \
  export TT_METAL_ARCH_NAME=blackhole && \
  export PYTHONPATH=$TT_METAL_HOME:$PYTHONPATH && \
  source ~/tt-vllm/tt_metal/setup-metal.sh && \
  python ~/tt-scratchpad/start-vllm-server.py \
    --model ~/models/Llama-3.1-8B-Instruct \
    --host 0.0.0.0 \
    --port 8000 \
    --max-model-len 65536 \
    --max-num-seqs 16 \
    --block-size 64
```

**ValidationError: Cannot find model module 'TTLlamaForCausalLM':**
This error means vLLM cannot find the TT model implementation. Solution:
```bash
# Use the starter script (Step 4) which registers TT models
python ~/tt-scratchpad/start-vllm-server.py --model ~/models/Llama-3.1-8B-Instruct
```

**Why this happens:** vLLM needs to explicitly register TT models using `ModelRegistry.register_model()` before starting. The starter script does this automatically. Do NOT call `python -m vllm.entrypoints.openai.api_server` directly - it will fail because TT models aren't registered.

**Verify your starter script exists:**
```bash
ls -la ~/tt-scratchpad/start-vllm-server.py
# If missing, use the extension button "Create vLLM Server Starter Script" in Lesson 6
```

**If you encounter other import errors (e.g., "No module named 'xyz'"):**
```bash
# Install the missing package
source ~/tt-vllm-venv/bin/activate
pip install <missing-package-name>
```

**Other import errors:**
```bash
# Reinstall vLLM in the venv
source ~/tt-vllm-venv/bin/activate
cd ~/tt-vllm
pip install -e . --extra-index-url https://download.pytorch.org/whl/cpu
```

**Virtual environment issues:**
```bash
# Recreate the venv if it's corrupted
rm -rf ~/tt-vllm-venv
cd ~/tt-vllm
python3 -m venv ~/tt-vllm-venv
source ~/tt-vllm-venv/bin/activate
pip install --upgrade pip
export vllm_dir=$(pwd)
source $vllm_dir/tt_metal/setup-metal.sh
pip install fairscale termcolor loguru blobfile fire llama-models==0.0.48
pip install -e . --extra-index-url https://download.pytorch.org/whl/cpu
```

**Slow inference:**
- Check `--max-num-seqs` setting
- Monitor GPU/NPU utilization
- Reduce `--max-model-len` if not needed

**Out of memory:**
- Reduce `--max-num-seqs`
- Reduce `--max-model-len`
- Close other programs

---

## Alternative: Use TT-Jukebox (Recommended!)

**Struggling with version mismatches or hardware compatibility?**

TT-Jukebox is a standalone tool that automates everything in this lesson:
- âœ… Detects your hardware automatically
- âœ… Finds compatible models for your device
- âœ… Sets correct tt-metal and vLLM commits
- âœ… Generates setup scripts with exact versions
- âœ… Provides tested vLLM flags
- âœ… Downloads models automatically

**Quick start with Jukebox:**
```bash
# Clone TT-Jukebox (standalone repository)
git clone https://github.com/tenstorrent/tt-jukebox.git ~/tt-jukebox
cd ~/tt-jukebox

# Find chat models for your hardware
python3 tt-jukebox.py chat

# Select a model â†’ generates setup script â†’ run it â†’ done!
```

**Why use Jukebox?**
- Eliminates version mismatch errors
- Uses official Tenstorrent-tested configurations
- Reproducible environments (share scripts with team)
- Supports experimental models and hardware
- Updates automatically from official specs

**When to use Jukebox:**
- First time setting up vLLM
- After updating tt-metal or vLLM
- Switching between different models
- Working on a team (share setup scripts)
- Trying experimental hardware (P100, P150)

See the [TT-Jukebox repository](https://github.com/tenstorrent/tt-jukebox) for the complete guide.

---

## What You Learned

âœ… How to install and configure vLLM for Tenstorrent
âœ… OpenAI-compatible API usage
âœ… Continuous batching for efficient serving
âœ… Streaming responses
âœ… Production deployment patterns
âœ… Performance monitoring and tuning

**Key takeaway:** vLLM bridges the gap between custom code and production deployment, giving you enterprise features while maintaining compatibility with standard APIs.

## Next Steps

**You've completed the walkthrough!** ğŸ‰

**Where to go from here:**

1. **Build Applications:**
   - Integrate with your existing services
   - Build chat interfaces
   - Create AI-powered features

2. **Optimize Performance:**
   - Tune batch sizes for your workload
   - Implement caching strategies
   - Monitor and optimize

3. **Scale Up:**
   - Deploy multiple instances
   - Add load balancing
   - Implement autoscaling

4. **Explore More Models:**
   - Try different Llama variants
   - Test Mistral, Qwen, etc.
   - Fine-tune for your use case

## Learn More

- **TT vLLM Fork:** [github.com/tenstorrent/vllm](https://github.com/tenstorrent/vllm/tree/dev)
- **vLLM Docs:** [docs.vllm.ai](https://docs.vllm.ai/en/latest/)
- **OpenAI API Reference:** [platform.openai.com/docs](https://platform.openai.com/docs/api-reference)
- **TT-Metal Docs:** [docs.tenstorrent.com](https://docs.tenstorrent.com/)

## Community & Support

- **GitHub Issues:** Report bugs and request features
- **Discord:** Join the Tenstorrent community
- **Documentation:** Check the tt-metal README

**Thank you for completing this walkthrough!** You now have the knowledge to build, deploy, and scale AI applications on Tenstorrent hardware. ğŸš€
